# Cache

## Where cache can occur

1. Browser
2. Reverse proxy
3. Application

The first two occur at the HTTP level.

Application-level caching will be more manual.

## Browser

### Cache-control

When the browser makes an HTTP request to the server, it stores the response in the cache (whether it's HTML, JSON, or any static file, etc.). Why? Because it received an HTTP header like `Cache-control: max-age=3600`. This means the next time the browser needs to make the same request, it will first check the cache and use the response for up to 3600 seconds.

However, if you directly enter the URL or manually refresh the page, the browser assumes you want the most recent version and ignores the cache, even if the header previously stored it. If the request is made via `fetch`, though, the cache will be used as expected.

There's also an additional header: `Cache-control: max-age=3600, stale-while-revalidate=60`. This means that for 3600 seconds, the browser will use cached data, but during the next 60 seconds after that period, it will serve the cached data while updating it in the background, making it seamless for the user who won’t experience any delay while waiting for fresh data.

A potential risk is the "dog pile effect," where too many clients request the same cached resource at once, potentially overwhelming the server. To mitigate this, a reverse proxy can be used.

### ETag

Instead of using a cache-control header, an ETag can be returned, for example, `ETag: "686897696a7c876b7e"`. This is a unique identifier derived from the response content, so if the content changes, the ETag changes as well.

On subsequent requests to the same resource, the browser will send the ETag in a header:

```http
GET /index.html HTTP/1.1
Host: www.example.com
If-None-Match: "686897696a7c876b7e"
```

The server will then respond only if the content has changed. If not, the server returns a `304 Not Modified`, saving bandwidth and computation by not sending the content again.

ETag verification is typically implemented on the server side via middleware, so it doesn't have to be written into every controller.

## Reverse proxy

Cache management can be offloaded to an intermediary (the reverse proxy) between the browser and the backend.

Browser -> Reverse proxy (e.g., Varnish, Cloudflare) -> Backend

In this setup, the backend sends a special header: `Cache-control: public, s-maxage=3600`.

This "public" cache (denoted by `s-`) can reside in the reverse proxy and be shared across clients. It can also be marked as "private," which would limit the cache to a specific client's browser.

This setup allows requests to be fulfilled by the reverse proxy, reducing the load on the backend.

However, one issue that can arise is the inability to invalidate a portion of the cache—for instance, if some data needs frequent updating while other data doesn't. With a reverse proxy or browser cache, you can't handle such selective invalidation.

## Application

Application-level caching can be applied at different layers (controller, use case, repository), each with its own pros and cons. One possibility is to create a repository with caching logic, like `CacheUserRepository`.

Personally, I’d include caching at the controller level to cover everything. We could move the logic into middleware to avoid repetition or use a more sophisticated system. A cache key could be created based on variables received from the request using a pattern like Criteria or Specification.

Next, we would check if the cache key exists (using Redis, for example, or some other in-memory cache system). If the cache exists, return the cached result. If not, proceed with the full request and generate a new cache.

If we move caching into the use case (one level below the controller), we can make caching independent of the request itself, which makes it more granular. This allows any endpoint that uses the use case to benefit from the cache. In this scenario, we would serialize search criteria and ask an injected cache service (like Redis) if the data exists. If it does, return the cached data; otherwise, generate and cache the data.

However, this introduces infrastructure noise into domain code, which isn’t ideal even if the dependency is well inverted. Thanks to hexagonal architecture, use case collaborators won’t interfere with testing. Still, I’d avoid this because it introduces infrastructure semantics into the use case.

Another option is caching at the repository level. Each repository implementation would handle caching, and this seems less intrusive because both caching and the database are infrastructure-related. Moreover, databases can often be bottlenecks, making caching especially useful at this level.

However, coupling the database and cache (e.g., MySQL and Redis) in the repository can lead to strange failures—like Redis failing when you’re also querying the database. This introduces a bad smell, as now the repository has two responsibilities, which is similar to the previous problem.

### Explicit control

We can make caching explicit in the repository, such as with a `CachePostRepository`.

For example, you could have a `RedisCachePostRepository` that implements a `PostRepository` and decorates it with caching functionality. It would receive a Redis client and a `PostRepository` to decorate via constructor injection.

This `RedisCachePostRepository` acts as a decorator for the original repository, adding caching functionality.

This approach targets areas where caching is typically needed, without polluting controllers and use cases with infrastructure concerns. It also provides fine-grained caching near the data source.

You could also create a decorator at the use case or service level, where you’d have a use case without cache, but one with cache would wrap it, providing less granular caching if multiple repositories are injected into the service.
